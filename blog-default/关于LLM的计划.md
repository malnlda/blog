使用 LLM 的目标

目前阶段 LLM 模型和工具竞争异常激烈，弃用 deepseek-r1、投奔 gpt-o1 才没多久，就体验到 grok 3 的 deep search 检索结果快速且精确，claude sonnet 3.7 可以用「一杯瑞幸的钱」做出极高完成度且非常漂亮的网页和应用，gpt-4.5 具备更强大的世界知识、准确度和拟人度大幅提升，gemini 2.0 flash (image generation) 可以一句话修图、一句话生成数十张图片的分镜动作，gemini 2.0 也推出了 deep research，眼花缭乱，更不提 manus 等 agent 工具层出不穷搅动市场，以及明天（据传）就要发布 deepseep-r2 等等，极易激起用户对 ai 工具的 fomo 焦虑。在我看来，紧密关注 LLM 模型和工具的发展是必须的，但也不必频繁切换转投「阵营」。工具毕竟只是工具，应当关注自己的使用场景，明确使用目标，按需选择工具，例如：

1.LLM 作为快速研究工具

我们以前如果需要速读一本书，需要快速进入一个知识领域，需要快速了解一个主题下的整体分析，比较通用的办法是到豆瓣翻一篇书评、到知乎或公众号找一篇长文解析，找得到最好，找不到就得自己从头学起。而自从模型发展出 deep research 能力，以及类 deep research 的 agent 工具涌现，在满足「快速研究」目的上，很大范围内已经不需要依赖文章检索。例如，你可以向模型询问：「如果我想要通过 deep research / 学习理解《古代法》这本书的整体内容 / 研究全世界范围内律师使用 LLM 的发展情况 / 研究人身商业保险在家庭财富传承和婚姻家事纠纷领域的相关问题，我应该如何撰写提示词？」对模型生成的提示词结果，按需要做一些调整，比如「请你检索英语世界内容，并用中文输出 / 请你输出不少于两万中文字的结果」等，然后将整理好的提示词输入到 deep research 或类 deep search 的 agent 工具中，等待生成结果。测试发现，这一结果的质量，足以实现「快速研究」的目标，能够从中提取学习和写作灵感，甚至稍作修改即可输出成专业文章。

2.LLM 作为知识库工具

通用模型的知识过于宽泛，无法满足自己本专业获取知识和团队协作的需求，因而需要搭建知识库，实现对既有知识内容的快速检索、总结、分析……这已成为全网关于 LLM 讨论最多的话题，是最急切的需求，是大部分用户 fomo 焦虑最严重的话题。我们法律实务领域在此话题下的 fomo 焦虑度更为甚之，律师日常工作需要解答客户法律咨询，但对于每个问题都不敢随意回答，进而需要做大量的法律研究、案例检索，消耗大量时间。但同行们发现通用模型的答案并不靠谱，会产生编造法条、案例的「幻觉」，故此更为急切地需要能够解决法律知识库的模型工具。于是，新橙推出了 alphagpt，定价 5000 元/三年；熊猫
推出 deepseek 大模型一体机，价格不详；智合推出「deepseek 法律一体机，价格 50 万……

在我看来，对于任何一件互联网上炒作特别热门的事，都需要打开自己脑子里的「防骗警报器」，审视自己是否有必要打开付款码淌入这片浑水。通用模型难以用于做垂直领域知识库，主要源于两个原因：

第一，上下文长度限制。在单一对话中，为了「记住」前边说过的话，每向发一次对话要求，模型都会将之前的内容全部阅读一遍，如此就会消耗 tokens。对话内容越多，模型每次消耗的 tokens 就越大。由于每个模型支持的上下文 tokens 有限，如果对话整体内容超出限制，模型就会「忘记」之前内容，导致答复不精确。目前 deepseek-r1 上下文长度限制为 128k（api 为 64k），约对应 64k 中文字（api 为 32k），gpt-4o 为 128k，gemini-2.0-pro 为 200k。原理上模型需要对输入的内容做「向量化」解析，通俗理解（我也只懂通俗理解）是将自然语言转化为模型可识别的向量文本。而即便 64k 中文字看起来已经很多，但不同模型解析文本的能力有差异，或是选择只读每段首句，或是选择只读重点部分，一般也不会全文阅读，从而使得解析能力更不靠谱。

第二，使用习惯限制。尽管模型对于阅读上下文仍有限制，但模型能力的快速发展已经使得理解上下文的精准度逐渐提升。然而通用模型仍存在的另一个障碍，是用户的使用习惯。我们通常习惯将文本知识存入计算机文档或笔记管理工具中，用户希望的场景是自己所存储的内容能够原封不动的留在原处，然后接入模型能力对现有的知识库做检索分析。如果使用通用模型，则不得不每次都将内容打散再次上传，非常「麻烦」。而后又由于模型的上下文限制，每次能上传的内容不多，尤其法律从业者所希冀的法律法规库、案例库、实务指引库可能总量达到上百万乃至千万，当然是希望能够「一站式解决问题」。

那本地部署模型呢？也有两大限制问题：

其一，硬件限制。按自己现在的电脑配置，用 anythingllm 本地部署一个 deepseek-r1-7b 应该刚刚好，但即便是目前的 671b 满血版 r1，幻觉问题都严重到没法正常使用，推理过程惊艳但输出答案质量不高（据传明天发布 r2，非常期待），如果回退到 7b 版本，那还怎么用？如果要部署满血 671b 版本，听说苹果新出的八万块 m3 ultra 可以？但数据处理速度慢得感人。如果要堆显卡，据说需要 30~40 张 RTX 4090，价格感人。可问题是，有必要吗？

其二，向量转化能力限制。如前所说，模型识别、分析长文前，需要通过向量转化嵌入模型（通常是指 embedding 模型）将文本内容转化为计算机可识别的向量模型。即便是本地重金成功部署好了满血版 r1，或是使用第三方工具上传批量知识库，所面临的第一个问题就是如何将大量的知识库内容做向量转化从而满足模型精准输出的需求。所以即便部署好了满血版 r1，或是使用了第三方管理工具，仍要进一步选择本地部署较好的嵌入模型，模型的选择、转化质量等，同样又是导致知识库工具难以优质运转的限制问题。

因此，就目前阶段而言，我认为应该把自己从知识库 fomo 焦虑中跳脱出来，优先考虑一个更重要的问题：（1）对于既有知识，自己手上的垂直知识内容到底能有多少，是否已经到了自己无法消化、需要靠电子大脑帮助消化的程度？（2）对于未有知识，现有的工具是否已经不够用，到了需要花大价钱购买在线/本地知识库工具？—— 归结而言，抛开对工具的 fomo 焦虑，关注要点仍应当是：关注如何生产知识，而没到需要大规模整理知识的程度。







3.